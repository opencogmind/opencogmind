"use strict";(self.webpackChunkopencogmind=self.webpackChunkopencogmind||[]).push([[624],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>h});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function r(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),u=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},c=function(e){var t=u(e.components);return a.createElement(l.Provider,{value:t},e.children)},d="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,c=r(e,["components","mdxType","originalType","parentName"]),d=u(n),p=i,h=d["".concat(l,".").concat(p)]||d[p]||m[p]||o;return n?a.createElement(h,s(s({ref:t},c),{},{components:n})):a.createElement(h,s({ref:t},c))}));function h(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,s=new Array(o);s[0]=p;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r[d]="string"==typeof e?e:i,s[1]=r;for(var u=2;u<o;u++)s[u]=n[u];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}p.displayName="MDXCreateElement"},3135:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>r,toc:()=>u});var a=n(7462),i=(n(7294),n(3905));const o={sidebar_position:6},s="AGI: Capability Vs Autonomy",r={unversionedId:"causalai/Fundamentals/AutonomyVsCapability",id:"causalai/Fundamentals/AutonomyVsCapability",title:"AGI: Capability Vs Autonomy",description:"Capability Vs Autonomy",source:"@site/docs/causalai/Fundamentals/AutonomyVsCapability.md",sourceDirName:"causalai/Fundamentals",slug:"/causalai/Fundamentals/AutonomyVsCapability",permalink:"/causalai/Fundamentals/AutonomyVsCapability",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/causalai/Fundamentals/AutonomyVsCapability.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"AGI: Risk Assessment",permalink:"/causalai/Fundamentals/Risk-Assessment"},next:{title:"Sparks Of AGI: GPT-4",permalink:"/causalai/Research/Sparks-Of-AGI"}},l={},u=[{value:"Balancing Capability and Autonomy decides future of humanity!",id:"balancing-capability-and-autonomy-decides-future-of-humanity",level:3},{value:"Understanding Different levels of AI Autonomy in Simple Terms",id:"understanding-different-levels-of-ai-autonomy-in-simple-terms",level:3}],c={toc:u},d="wrapper";function m(e){let{components:t,...o}=e;return(0,i.kt)(d,(0,a.Z)({},c,o,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"agi-capability-vs-autonomy"},"AGI: Capability Vs Autonomy"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Capability Vs Autonomy",src:n(6188).Z,title:"Autonomous decisions made by AGI Might not be in the best of humans! ",width:"1600",height:"900"}),"\nThe main point here is that as AI becomes more advanced, especially AGI, we need to think carefully about how much independence it should have. This depends on the AI's ability to understand and interact in complex, human-like ways. It's also important to keep improving how humans and AI communicate and work together and make sure AGI is safe and aligns with human values and ethics."),(0,i.kt)("h3",{id:"balancing-capability-and-autonomy-decides-future-of-humanity"},"Balancing Capability and Autonomy decides future of humanity!"),(0,i.kt)("p",null,'Developing ways for humans and AI to work together effectively is a active of research.  When AI reaches the highest level of autonomy (called "AI as an Agent"), it means the AI can make decisions and act on its own, aligning with human goals and values. But even at this level, it should know when it\'s important to check in with humans for guidance or decisions. For some ways of interacting with AI, it\'s important that the AI is not just good at one thing but has a broader understanding. This is especially true for higher levels of autonomy, like when AI acts as a "Collaborator," "Expert," or "Agent."\nAt these levels, the AI needs to be good at understanding when it should ask for human help, understanding others\' thoughts and feelings, and having social skills.'),(0,i.kt)("p",null,"In the realm of Artificial Intelligence (AI), particularly Artificial General Intelligence (AGI), there's a crucial distinction between capabilities and autonomy. AI systems, including those aiming for AGI, operate within specific contexts defined by their interfaces, tasks, scenarios, and end-users. Once Autonomy is granted to an AGI system the risk profiles of such systems should be carefully studied and suitable mitigation strategies must in place."),(0,i.kt)("h3",{id:"understanding-different-levels-of-ai-autonomy-in-simple-terms"},"Understanding Different levels of AI Autonomy in Simple Terms"),(0,i.kt)("p",null,'Just like we have different types of AI that are really good at specific tasks (what the paper calls "Superhuman Narrow AI"), the same idea applies to AGI (Artificial General Intelligence). AGI is a type of AI that can understand and learn a wide range of things, much like a human. We might create an AGI that can do a lot of things on its own (autonomously), but we might choose not to let it operate by itself all the time. Instead, we could decide how much freedom it should have based on the situation.\n',(0,i.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/2311.02462.pdf"},"Six Levels of Autonomy")," introduced in the ",(0,i.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/2311.02462.pdf"},"paper")," correlate with five levels of performance (Emerging, Competent, Expert, Virtuoso, and Superhuman) of AGI. For instance, in self-driving vehicles, even when Level 5 Self-Driving technology (SAE International, 2021) is available, there might be scenarios where a Level 0 (No Automation: No AI\u201d Paradigm) vehicle is preferable. This could be for teaching new drivers, for the enjoyment of driving enthusiasts, for driver\u2019s licensing exams, or in extreme weather conditions where technology might fail. Although Higher levels of autonomy become possible as AGI capabilities progress lower levels of autonomy might still be preferred for certain tasks, especially for safety reasons. The journey towards AGI is not just about advancing capabilities but also about making thoughtful decisions regarding autonomy. The choices we make in designing human-AI interactions will significantly impact the safety and responsible deployment of AI technologies. As we progress, it's essential to remember that sometimes, the best choice might be to not use AI at all."))}m.isMDXComponent=!0},6188:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/MultitaskingVsPowerSaveMode-a2de3d652226f883d4daed750e9befdc.png"}}]);