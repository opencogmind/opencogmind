"use strict";(self.webpackChunkopencogmind=self.webpackChunkopencogmind||[]).push([[6004],{3905:(e,t,s)=>{s.d(t,{Zo:()=>u,kt:()=>h});var i=s(7294);function a(e,t,s){return t in e?Object.defineProperty(e,t,{value:s,enumerable:!0,configurable:!0,writable:!0}):e[t]=s,e}function n(e,t){var s=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),s.push.apply(s,i)}return s}function r(e){for(var t=1;t<arguments.length;t++){var s=null!=arguments[t]?arguments[t]:{};t%2?n(Object(s),!0).forEach((function(t){a(e,t,s[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(s)):n(Object(s)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(s,t))}))}return e}function o(e,t){if(null==e)return{};var s,i,a=function(e,t){if(null==e)return{};var s,i,a={},n=Object.keys(e);for(i=0;i<n.length;i++)s=n[i],t.indexOf(s)>=0||(a[s]=e[s]);return a}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(i=0;i<n.length;i++)s=n[i],t.indexOf(s)>=0||Object.prototype.propertyIsEnumerable.call(e,s)&&(a[s]=e[s])}return a}var l=i.createContext({}),c=function(e){var t=i.useContext(l),s=t;return e&&(s="function"==typeof e?e(t):r(r({},t),e)),s},u=function(e){var t=c(e.components);return i.createElement(l.Provider,{value:t},e.children)},d="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},m=i.forwardRef((function(e,t){var s=e.components,a=e.mdxType,n=e.originalType,l=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),d=c(s),m=a,h=d["".concat(l,".").concat(m)]||d[m]||p[m]||n;return s?i.createElement(h,r(r({ref:t},u),{},{components:s})):i.createElement(h,r({ref:t},u))}));function h(e,t){var s=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var n=s.length,r=new Array(n);r[0]=m;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o[d]="string"==typeof e?e:a,r[1]=o;for(var c=2;c<n;c++)r[c]=s[c];return i.createElement.apply(null,r)}return i.createElement.apply(null,s)}m.displayName="MDXCreateElement"},3854:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>n,metadata:()=>o,toc:()=>c});var i=s(7462),a=(s(7294),s(3905));const n={sidebar_position:5},r="AGI: Risk Assessment",o={unversionedId:"agi/Fundamentals/Risk-Assessment",id:"agi/Fundamentals/Risk-Assessment",title:"AGI: Risk Assessment",description:"AGI  or even for that matter Narrow AI is viewed by some experts  as a concept for identifying the point when there are",source:"@site/docs/agi/Fundamentals/Risk-Assessment.md",sourceDirName:"agi/Fundamentals",slug:"/agi/Fundamentals/Risk-Assessment",permalink:"/agi/Fundamentals/Risk-Assessment",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/agi/Fundamentals/Risk-Assessment.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5},sidebar:"mySidebar",previous:{title:"Evaluating AGI",permalink:"/agi/Fundamentals/Evaluating-AGI"},next:{title:"AGI: Capability Vs Autonomy",permalink:"/agi/Fundamentals/AutonomyVsCapability"}},l={},c=[{value:"Risk Introduction with Advanced AGI:",id:"risk-introduction-with-advanced-agi",level:4},{value:"Risks at the &#39;Expert AGI&#39; Level:",id:"risks-at-the-expert-agi-level",level:4},{value:"Higher-Level Risks (&#39;Virtuoso AGI&#39; and &#39;ASI&#39;):",id:"higher-level-risks-virtuoso-agi-and-asi",level:4},{value:"Systemic Risks:",id:"systemic-risks",level:4}],u={toc:c},d="wrapper";function p(e){let{components:t,...s}=e;return(0,a.kt)(d,(0,i.Z)({},u,s,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"agi-risk-assessment"},"AGI: Risk Assessment"),(0,a.kt)("p",null,"AGI  or even for that matter Narrow AI is viewed by some experts  as a concept for identifying the point when there are\n",(0,a.kt)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=144uOfr4SYA&t=132s&pp=ygUaZGViYXRlIEFJIEV4aXN0ZW50aWFsIFJJc2s%3D"},"extreme existential risks"),", as some speculate that AGI systems might be able\nto deceive and manipulate, accumulate resources, advance goals, and outwit humans\nin broad domains, displace humans from key roles by recursively self-improving.\nAI 'godfather' Geoffrey Hinton ",(0,a.kt)("a",{parentName:"p",href:"https://www.bbc.com/news/av/world-us-canada-65453192"}," tells the BBC ")," of AI dangers after he quits Google.\n",(0,a.kt)("a",{parentName:"p",href:"https://www.bbc.com/news/world-us-canada-65452940"},"Geoffrey Hinton says")),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},(0,a.kt)("em",{parentName:"p"},"\"I've come to the conclusion that the kind of intelligence we're developing is very different from the intelligence we have.We're biological systems and these are digital systems. And the big difference is that with digital systems, you have many copies of the same set of weights, the same model of the world. \"And all these copies can learn separately but share their knowledge instantly. So it's as if you had 10,000 people and whenever one person learnt something, everybody automatically knew it. And that's how these chatbots can know so much more than any one person.\"?"))),(0,a.kt)("p",null,"Another expert Yoshua Bengio, a collaborator with Geoffery Hilton, highlights the need for ",(0,a.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2310.17688"},"Managing AI Risks"),". The risks are identified by varying levels of AGI by ",(0,a.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/2311.02462.pdf"},"researchers at Google Deepmind"),'. Different levels of AI capability can bring different types of risks. As AI progresses from basic to more sophisticated stages (like from "Emerging AGI" to "Expert AGI" and ultimately to "ASI" - Advanced Superintelligence), new risks arise. For example, at the "Expert AGI" stage, there might be risks related to job losses and economic changes. In contrast, at higher levels like "Virtuoso AGI" and "ASI," there are concerns about existential risks, such as an AI deceiving humans to achieve its goals. The more "x-systemic risks," like destabilizing international relations, significantly if AI development outpaces regulation, need deeper collaboration and understanding by the policymakers within various countries focusing on immediate and long-term risks.'),(0,a.kt)("p",null,"With varying capability levels brings risks associated, The risks are summarized at various levels of Artificial General Intelligence (AGI) below:"),(0,a.kt)("h4",{id:"risk-introduction-with-advanced-agi"},"Risk Introduction with Advanced AGI:"),(0,a.kt)("p",null,"As AI progresses towards Artificial Superintelligence (ASI), new risks arise. These include misuse risks, risks of AI not aligning with human values or goals (alignment risks), and risks related to the structure of systems and society (structural risks)."),(0,a.kt)("h4",{id:"risks-at-the-expert-agi-level"},"Risks at the 'Expert AGI' Level:"),(0,a.kt)("p",null,"At this level, AI might cause economic disruption and job losses as machines replace human labor in more industries. However, it could also reduce risks seen at lower levels, like errors in task execution."),(0,a.kt)("h4",{id:"higher-level-risks-virtuoso-agi-and-asi"},"Higher-Level Risks ('Virtuoso AGI' and 'ASI'):"),(0,a.kt)("p",null,"At these advanced stages, there's concern about existential risks (x-risks). For instance, highly capable AI might deceive humans to achieve wrongly specified goals."),(0,a.kt)("h4",{id:"systemic-risks"},"Systemic Risks:"),(0,a.kt)("p",null,"If AI progresses too quickly, it could lead to international instability, especially if one nation achieves ASI before others. This could give that nation a significant geopolitical or military advantage, creating complex structural risks.\nOverall, considering both the capabilities of AI at each level and the potential societal impacts and risks is a good framework for policy makers to make inform decisions about the regulatory frameworks."))}p.isMDXComponent=!0}}]);