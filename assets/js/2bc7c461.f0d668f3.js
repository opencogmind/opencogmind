"use strict";(self.webpackChunkopencogmind=self.webpackChunkopencogmind||[]).push([[9165],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>h});var n=a(7294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},f=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=c(a),f=i,h=d["".concat(l,".").concat(f)]||d[f]||u[f]||r;return a?n.createElement(h,o(o({ref:t},p),{},{components:a})):n.createElement(h,o({ref:t},p))}));function h(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=a.length,o=new Array(r);o[0]=f;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[d]="string"==typeof e?e:i,o[1]=s;for(var c=2;c<r;c++)o[c]=a[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}f.displayName="MDXCreateElement"},2463:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var n=a(7462),i=(a(7294),a(3905));const r={sidebar_position:1},o="Why need AI Safety?",s={unversionedId:"aisafety/Why-Need-AI-Safety",id:"aisafety/Why-Need-AI-Safety",title:"Why need AI Safety?",description:"Artificial Intelligence (AI) has evolved rapidly, raising significant concerns about its potential catastrophic risks. Understanding and managing these risks is crucial for harnessing AI's potential for societal betterment.",source:"@site/docs/aisafety/Why-Need-AI-Safety.md",sourceDirName:"aisafety",slug:"/aisafety/Why-Need-AI-Safety",permalink:"/aisafety/Why-Need-AI-Safety",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/aisafety/Why-Need-AI-Safety.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"mySidebar2",next:{title:"Why AI Safety?",permalink:"/aisafety/AI-Safety-101/AI-Safety-Intro"}},l={},c=[{value:"1. Malicious Use of AI",id:"1-malicious-use-of-ai",level:3},{value:"2. The Race to develop AI",id:"2-the-race-to-develop-ai",level:3},{value:"3. Organizational Risks",id:"3-organizational-risks",level:3},{value:"4. AI turning to Rogue AIs",id:"4-ai-turning-to-rogue-ais",level:3},{value:"Organizations Working on AI Safety",id:"organizations-working-on-ai-safety",level:3},{value:"AI Safety Communities",id:"ai-safety-communities",level:3},{value:"Private corporations and the AI Safety Guidelines and considerations are listed below:",id:"private-corporations-and-the-ai-safety-guidelines-and-considerations-are-listed-below",level:3}],p={toc:c},d="wrapper";function u(e){let{components:t,...a}=e;return(0,i.kt)(d,(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"why-need-ai-safety"},"Why need AI Safety?"),(0,i.kt)("p",null,"Artificial Intelligence (AI) has evolved rapidly, raising significant concerns about its potential catastrophic risks. Understanding and managing these risks is crucial for harnessing AI's potential for societal betterment."),(0,i.kt)("h3",{id:"1-malicious-use-of-ai"},"1. Malicious Use of AI"),(0,i.kt)("p",null,"The intentional misuse of AI poses severe threats, including ",(0,i.kt)("a",{parentName:"p",href:"https://www.rand.org/pubs/research_reports/RRA2977-1.html"},"bioterrorism"),", the spread of uncontrolled AI agents, and AI-driven propaganda, censorship, and surveillance. To counter these risks, the paper suggests enhancing biosecurity, restricting access to dangerous AI models, and imposing legal liabilities on AI developers for damages caused by their systems."),(0,i.kt)("h3",{id:"2-the-race-to-develop-ai"},"2. The Race to develop AI"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},(0,i.kt)("em",{parentName:"p"},'Russian President Vladimir Putin, "Whoever becomes the leader in artificial intelligence will become the ruler of the world.'))),(0,i.kt)("p",null,"The ",(0,i.kt)("a",{parentName:"p",href:"https://www.ncuscr.org/podcast/us-china-ai-race/"},"race to develop AI"),", fueled by the game-theoretic dynamics of competition, could result in the rushed and unsafe rollout of these technologies. This particularly concerns the use of AI in military applications, like autonomous weaponry and cyber-warfare, raising the possibility of wars conducted by automated systems. Additionally, corporations may focus on AI advancement at the expense of safety considerations, posing risks of widespread job loss and excessive reliance on AI. The article calls for the implementation of safety standards, global cooperation, and the public governance of general-purpose AI systems to counter these potential dangers."),(0,i.kt)("h3",{id:"3-organizational-risks"},"3. Organizational Risks"),(0,i.kt)("p",null,"Similar to historical accidents like Chernobyl and the Challenger disaster, AI development organizations face the risk of catastrophic accidents.\nThe ",(0,i.kt)("a",{parentName:"p",href:"https://www.britannica.com/event/Chernobyl-disaster"},"Chernobyl disaster"),", the worst in nuclear power generation history, occurred at the Chernobyl nuclear power station in the Soviet Union in 1986. It caused initial explosions that killed between 2 and 50 people, and dozens more suffered serious radiation sickness, some of whom later also died.\nThe ",(0,i.kt)("a",{parentName:"p",href:"https://www.britannica.com/event/Challenger-disaster"},"Challenger disaster")," happened on January 28, 1986, when the U.S. Space Shuttle Challenger broke apart 73 seconds after liftoff, killing all seven crew members aboard. This tragedy was caused by a failure of the O-ring seals in the right solid rocket booster\u200b.\nIssues such as weak safety cultures, accidental AI leaks, and suppression of internal concerns about AI risks are highlighted. Solutions include improving organizational cultures, establishing internal and external audits, and implementing robust information security."),(0,i.kt)("h3",{id:"4-ai-turning-to-rogue-ais"},"4. AI turning to Rogue AIs"),(0,i.kt)("p",null,"Yoshua Bengio expresses ",(0,i.kt)("a",{parentName:"p",href:"https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/"},"concerns")," over losing control of AIs, especially as they surpass human intelligence. Rogue AI's are defined as ",(0,i.kt)("em",{parentName:"p"},"AI refers to an autonomous AI system capable of actions that might cause catastrophic harm to a significant portion of the human population, posing a threat to our societies, potentially our species, and even the entire biosphere"),".  Problems like optimizing flawed objectives, goal drift, and potential deception by AIs are identified. ",(0,i.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/2306.12001.pdf"},"The paper")," proposes research to understand and ensure the controllability of AI systems.\nThe paper ",(0,i.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/2306.12001.pdf"},"An Overview ofCatastrophic AI Risks")," uses illustrative scenarios to demonstrate how these risks could lead to catastrophic outcomes, emphasizing that while the risks are severe, they are not insurmountable. Proactive risk management can help in realizing the benefits of AI while minimizing potential dangers.\nIn essence, the paper from the ",(0,i.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/2306.12001.pdf"},"Center for AI Safety"),", authored by Dan Hendrycks, Mantas Mazeika, and Thomas Woodside, serves as a crucial guide for understanding and addressing the potential catastrophic impacts of AI, aiming for a safer, AI-integrated future\u200b\u200b."),(0,i.kt)("h3",{id:"organizations-working-on-ai-safety"},"Organizations Working on AI Safety"),(0,i.kt)("p",null,"A list of organizations dedicated to ensuring the safe and ethical development of artificial intelligence, along with their focus and commitment to ensuring the safe, secure, and ethical development and use of artificial intelligence. The organizations consider on various aspects of AI Safety such as\n-",(0,i.kt)("em",{parentName:"p"},"research"),",\n-",(0,i.kt)("em",{parentName:"p"},"policy-making"),",\n-",(0,i.kt)("em",{parentName:"p"},"practical applications, and public engagement")," to address the potential risks associated with AI technologies. The goal is to harness the benefits of AI while mitigating its risks, ensuring that AI systems are trustworthy, reliable, and beneficial to society.\n",(0,i.kt)("a",{parentName:"p",href:"https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute"},"U.S. Artificial Intelligence Safety Institute (NIST)"),"\nHosted a workshop to engage in a conversation about AI safety, attracting a significant number of participants.NIST Seeks collaborators for a consortium to build the ",(0,i.kt)("a",{parentName:"p",href:"https://www.nist.gov/news-events/news/2023/11/nist-seeks-collaborators-consortium-supporting-artificial-intelligence"},"foundation for trustworthy AI systems"),"\n",(0,i.kt)("a",{parentName:"p",href:"https://www.safe.ai/about"},"CAIS - Safe"),"\nFocuses on reducing societal-scale risks from AI through field-building and research, with teams dedicated to conceptual and empirical AI safety research. The center educates the public about AI Safety with ",(0,i.kt)("a",{parentName:"p",href:"https://course.mlsafety.org/"},"Course on AI Safety"),"\n",(0,i.kt)("a",{parentName:"p",href:"https://aisafety.stanford.edu/"},"Stanford Center for AI Safety"),"\nAims to develop rigorous techniques for building safe and trustworthy AI systems, thereby facilitating their successful adoption in society.\n",(0,i.kt)("a",{parentName:"p",href:"https://www.dhs.gov/ai/promoting-ai-safety-and-security"},"Homeland Security - Promoting AI Safety and Security"),"\nPlays a critical role in ensuring AI use is safe and secure nationwide, following a government-wide approach."),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://apartresearch.com/"},"AI Safety Research Organization"),"\nConducts research on AI Safety and policy considerations\n",(0,i.kt)("a",{parentName:"p",href:"https://www.vox.com/future-perfect/2022/8/10/23298108/ai-dangers-ethics-alignment-present-future-risk"},"Vox - AI bias and AI safety teams"),"\nDiscusses the division between two factions working to prevent AI dangers, focusing on AI bias and safety.\n",(0,i.kt)("a",{parentName:"p",href:"https://www.whitehouse.gov/wp-content/uploads/2023/07/Ensuring-Safe-Secure-and-Trustworthy-AI.pdf"},"White House Document on AI Safety"),"\nAddresses trust and safety teams, advancing AI safety research, privacy, protecting children, and managing the risks of AI."),(0,i.kt)("h3",{id:"ai-safety-communities"},"AI Safety Communities"),(0,i.kt)("p",null,"A big community of people interested in AI alignment, with channels ranging from general topics to specific fields to local ",(0,i.kt)("a",{parentName:"p",href:"https://coda.io/@alignmentdev/alignmentecosystemdevelopment"},"groups")),(0,i.kt)("h3",{id:"private-corporations-and-the-ai-safety-guidelines-and-considerations-are-listed-below"},"Private corporations and the AI Safety Guidelines and considerations are listed below:"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://openai.com/blog/our-approach-to-ai-safety"},"OpenAI"),"\nCommitted to keeping powerful AI safe and broadly beneficial, focusing on increasing productivity, enhancing creativity, and offering tailored learning experiences.\n",(0,i.kt)("a",{parentName:"p",href:"https://www.anthropic.com/index/core-views-on-ai-safety"},"Antropic View on AI Safety"),"\n",(0,i.kt)("a",{parentName:"p",href:"https://80000hours.org/problem-profiles/artificial-intelligence/"},"80,000 Hours - Preventing an AI-related catastrophe"),"\nFocuses on AI safety as a means to affect the long-run future, acknowledging the emotional difficulty of prioritizing this over immediate global problems."))}u.isMDXComponent=!0}}]);